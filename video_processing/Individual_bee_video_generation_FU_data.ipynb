{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a890292e",
   "metadata": {},
   "source": [
    "# Notebook to generate videos cropped to area around a bee for FU (BeesBook) dataset\n",
    "\n",
    "This code could be extended to automatically merge consecutive video of the same bee if no frame-gap exists between the extracted videos. Right now manual merging of videos is necessary if videos longer than one minute are required (as raw beesbook input videos are one minute long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf01f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import bb_behavior.utils.images\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import imageio\n",
    "import skimage.io\n",
    "from typing import Tuple\n",
    "import joblib\n",
    "import math\n",
    "import logging\n",
    "from video_utils import CustomVideoManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c32696",
   "metadata": {},
   "outputs": [],
   "source": [
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='indiv_video_gen_fu_data.log',\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# write info to console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(logging.Formatter(\n",
    "    '%(levelname)s - %(message)s'\n",
    "))\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Starting video generation ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046022f",
   "metadata": {},
   "source": [
    "## Set global variables\n",
    "\n",
    "| Variable | Explanation |\n",
    "| --- | --- |\n",
    "| OVERWRITE_ORIENT_ERRORS_IF_EXIST | Whether to overwrite orientation errors (by bee) file if it exists |\n",
    "| APPLY_BODY_MASKS | Whether to apply a body mask in each extracted video, which masks the area around the bee (liberally) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_ORIENT_ERRORS_IF_EXIST = False\n",
    "APPLY_BODY_MASKS = False\n",
    "\n",
    "\n",
    "FRAME_RATE = 6 \n",
    "N_JOBS = 8     # number of parallel jobs for frame extraction\n",
    "\n",
    "VIDEO_ROOT = '/mnt/windows-ssd/BeesData/'\n",
    "CACHE_PATH = '/mnt/windows-ssd/BeesData/tmp/'\n",
    "VIDEO_OUTPUT_PATH = '/mnt/windows-ssd/BeesData/videos_single_bees/'\n",
    "PARQUET_DIR = \"/mnt/windows-ssd/BeesData/parquet/Hive_B/\"\n",
    "ORIENT_ERRORS_FILE = \"/mnt/windows-ssd/trophallaxis_detection_code/data/orientation_errors_fu/orientation_errors.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d07ee",
   "metadata": {},
   "source": [
    "# Load trajectory parquet files into dataframe and prepare data\n",
    "\n",
    "Currently only videos of cam-2 are extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ed856",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_files = []\n",
    "if os.path.exists(PARQUET_DIR):\n",
    "    parquet_files.extend([\n",
    "        os.path.join(PARQUET_DIR, f) for f in os.listdir(PARQUET_DIR) if f.endswith('.parquet')\n",
    "    ])\n",
    "dfs = [pd.read_parquet(parquet_file) for parquet_file in parquet_files]\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df = df[df.cam_id == 2]\n",
    "\n",
    "# x and y pixel values are swapped in the parquet files\n",
    "df[['x_pixels','y_pixels']] = df[['y_pixels','x_pixels']]\n",
    "df['orientation_hive'] = df['orientation_hive'].to_numpy() - np.pi / 2\n",
    "\n",
    "# add 2 * pi to all negative values\n",
    "df.loc[df[\"orientation_hive\"] < 0, \"orientation_hive\"] += 2 * np.pi\n",
    "\n",
    "df_all_data = df.copy()\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f34fa7",
   "metadata": {},
   "source": [
    "# Combine the dataframe with the video file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8216d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_manager = CustomVideoManager(VIDEO_ROOT, CACHE_PATH, VIDEO_OUTPUT_PATH, max_workers=16)\n",
    "video_manager.clear_video_cache()\n",
    "\n",
    "\n",
    "frame_id_cam_dfs = []\n",
    "for cam_id in df['cam_id'].unique():\n",
    "    df_cam = df[df['cam_id'] == cam_id].copy()\n",
    "    df_cam.reset_index(inplace=True)\n",
    "\n",
    "    # Get video metadata\n",
    "    videos_df = video_manager.get_all_video_files_for_cam(cam_id, df_cam)\n",
    "\n",
    "    if videos_df.empty:\n",
    "        logger.info(f\"No video files found for df_cam with cam_id {cam_id}\")\n",
    "        continue\n",
    "\n",
    "    # Sort dataframes by time\n",
    "    df_cam.sort_values('timestamp', inplace=True)\n",
    "    videos_df.sort_values('start_time', inplace=True)\n",
    "    \n",
    "    if 'video_filename' in df_cam.columns:\n",
    "        df_cam = df_cam.drop(columns='video_filename')\n",
    "\n",
    "    # Merge using merge_asof\n",
    "    df_cam = pd.merge_asof(df_cam, videos_df,\n",
    "                           left_on='timestamp',\n",
    "                           right_on='start_time',\n",
    "                           direction='backward',\n",
    "                           tolerance=pd.Timedelta(seconds=60))\n",
    "\n",
    "    # if not all data in the parquet file(s) has matching videos then timestamp, start_time and end_time are NaN after merge, so remove these rows\n",
    "    df_cam = df_cam[df_cam.start_time.notna()]\n",
    "    \n",
    "    # Calculate frame indices\n",
    "    df_cam['time_diff'] = (df_cam['timestamp'] - df_cam['start_time']).dt.total_seconds()\n",
    "    df_cam['frame_index'] = (df_cam['time_diff'] * FRAME_RATE).astype(int)\n",
    "\n",
    "    # Map video identifiers to unique integers\n",
    "    unique_videos = df_cam['video_filename'].unique()\n",
    "    video_id_map = {video: idx for idx, video in enumerate(unique_videos)}\n",
    "\n",
    "    # Assign video ID\n",
    "    df_cam['video_id'] = df_cam['video_filename'].map(video_id_map)\n",
    "\n",
    "    # Assign frame_id using the DataFrame index\n",
    "    df_cam['frame_id'] = df_cam['index']\n",
    "\n",
    "    # Calculate frame_id using video ID as an offset\n",
    "    df_cam['frame_id'] = df_cam['video_id'] * 360 + df_cam['frame_index']\n",
    "\n",
    "    # dataframe with all detections sorted by video, then by bee, then by timestamp\n",
    "    df_cam.sort_values(['video_filename', 'bee_id', 'timestamp'], inplace=True)\n",
    "\n",
    "    # Create a DataFrame mapping frame IDs to video names and frame indices\n",
    "    frame_id_cam_df = df_cam[['frame_id', 'cam_id', 'video_filename', 'frame_index', 'x_pixels', 'y_pixels', 'bee_id', 'orientation_hive', 'timestamp']].drop_duplicates(['frame_id', 'cam_id', 'video_filename', 'frame_index', 'bee_id'])\n",
    "    frame_id_cam_df['frame_id_unique'] = frame_id_cam_df['frame_id'] + (frame_id_cam_df['frame_id'].max()+1)*frame_id_cam_df['cam_id']\n",
    "    frame_id_cam_df = frame_id_cam_df.set_index('frame_id_unique')\n",
    "\n",
    "    frame_id_cam_dfs.append(frame_id_cam_df)\n",
    "\n",
    "frame_id_df = pd.concat(frame_id_cam_dfs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af74250",
   "metadata": {},
   "source": [
    "# Do orientation correction for all bees based on their movement\n",
    "Some of the bees have misaligned bee tags that don't face to the front of the bee. This cell aims to correct that by factoring in the movement directions of the bees.\n",
    "\n",
    "Assume that bees move forwards so the movement vector is the same as the orientation vector. For each bee look at the movement direction between each pair of consecutive timestamps $t_n$ and $t_{n+1}$ if the next timestamp $t_{n+1}$ is also from the next frame in the original data and if there is enough movement over the next 3 timestamps (between $t_n$ and $t_{n+3}$). Create a vector from the movement between $t_n$ and $t_{n+1}$ and calculate the angle difference to the orientation vector in the data at time $t_n$. These orientation errors are saved in a list. If this list is longer than 50 elements for a bee then it is assumed to be long enough for orientation correction. The list is sorted and the top 45% and bottom 45% of values are discarded and the mean of the remaining elements is calculated. This value is used as the final orientation error for the bee and gets applied to all the orientation values of this bee in the dataframe.\n",
    "\n",
    "In the videos the bees don't move exactly forwards in most cases and also sometimes bend their heads in a different direction than their bodies but over a long enough period of time the average of the differences of the movement direction vectors to the orientation in the data gives a good orientation error estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120cf6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_difference(\n",
    "    a: float, \n",
    "    b: float\n",
    ") -> float:\n",
    "    \"\"\"Calculates the angle difference between two angles\n",
    "\n",
    "    Args:\n",
    "        a (float): Angle in radians\n",
    "        b (float): Angle in radians\n",
    "\n",
    "    Returns:\n",
    "        float: Angle difference\n",
    "    \"\"\"\n",
    "    diff = (b - a + math.pi) % (2 * math.pi) - math.pi\n",
    "    return diff\n",
    "\n",
    "\n",
    "def get_orientation_error_for_detection(\n",
    "    i: int, \n",
    "    x_arr: np.ndarray, \n",
    "    y_arr: np.ndarray, \n",
    "    timestamps: np.ndarray, \n",
    "    orientations: np.ndarray, \n",
    "    next_detections_to_check_cnt: int\n",
    "):\n",
    "    \"\"\"Calculate the orientation error for a bee at a certain timestamp based on its orientation \n",
    "       and movement over the next few frames. If not enough movement happens over the next few frames or if\n",
    "       the very next detection is not also the next frame in the original video then don't use this timestamp\n",
    "       for orientation correction and return None\n",
    "\n",
    "    Args:\n",
    "        i (int): Detection index of the frame for this bee\n",
    "        x_arr (np.ndarray): x-pixel values of detections for the bee\n",
    "        y_arr (np.ndarray): y-pixel values of detections for the bee\n",
    "        timestamps (np.ndarray): Timestamps of detections for the bee\n",
    "        orientations (np.ndarray): Orientation values of detections for the bee\n",
    "        next_detections_to_check_cnt (int): How many next detections should be checked for minimum movement\n",
    "\n",
    "    Returns:\n",
    "        float: Orientation error\n",
    "    \"\"\"\n",
    "\n",
    "    x = x_arr[i]\n",
    "    y = y_arr[i]\n",
    "    timestamp = timestamps[i]\n",
    "    orientation_in_data = orientations[i]\n",
    "\n",
    "    # L2 norm (distance of travel)\n",
    "    total_travel_dist = np.hypot(\n",
    "        x_arr[i + next_detections_to_check_cnt] - x, \n",
    "        y_arr[i + next_detections_to_check_cnt] - y\n",
    "    )\n",
    "    # if too little movement over the next detections, then don't use the heading for orientation correction\n",
    "    if total_travel_dist < 5 * next_detections_to_check_cnt:\n",
    "        return None\n",
    "\n",
    "    # check the next timestep\n",
    "    next_x = x_arr[i + 1]\n",
    "    next_y = y_arr[i + 1]\n",
    "    next_timestamp = timestamps[i + 1]\n",
    "    next_orientation_in_data = orientations[i + 1]\n",
    "\n",
    "    # if timestep too large (not consecutive frames) don't use the heading for orientation correction\n",
    "    if np.timedelta64(next_timestamp - timestamp, 'ms') > 200:\n",
    "        return None\n",
    "    \n",
    "    heading_angle_rad_current = math.atan2((next_y - y), (next_x - x))\n",
    "    \n",
    "    if heading_angle_rad_current < 0:\n",
    "        heading_angle_rad_current += 2 * math.pi\n",
    "        \n",
    "    heading_diff = abs(angle_difference(heading_angle_rad_current, next_orientation_in_data))\n",
    "    if heading_diff > math.pi:\n",
    "        heading_angle_rad_current -= math.pi\n",
    "        heading_angle_rad_current += 2 * math.pi if heading_angle_rad_current < 0 else 0\n",
    "\n",
    "    orientation_error = angle_difference(orientation_in_data, heading_angle_rad_current); \n",
    "    return orientation_error\n",
    "\n",
    "\n",
    "def get_all_orientation_errors(\n",
    "    next_detections_to_check_cnt: int = 3, \n",
    "    min_required_errors: int = 50\n",
    ") -> dict:\n",
    "    \"\"\"Calculates orientation errors for all bees, where enough frames can be found\n",
    "       with movement happens over the next frames\n",
    "\n",
    "    Args:\n",
    "        next_detections_to_check_cnt (int, optional): How many next detections should be checked for minimum movement. \n",
    "            Defaults to 3.\n",
    "        min_required_errors (int, optional): Minimum number of required errors for a bee to calculate the orientation error. \n",
    "            Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with bee-IDs as keys and orientation errors as values\n",
    "    \"\"\"\n",
    "    logger.info(\"Calculating orientation errors...\")\n",
    "    relevant_bee_ids = frame_id_df.bee_id.unique()\n",
    "    df_relevant_data = df_all_data[df_all_data['bee_id'].isin(relevant_bee_ids)]\n",
    "\n",
    "    df_relevant_data.sort_values('timestamp', inplace=True)\n",
    "    detections_by_bee = df_relevant_data.groupby(\"bee_id\", sort=True)\n",
    "\n",
    "    orientation_error_by_bee = {}\n",
    "    for _, bee_detections in detections_by_bee:\n",
    "        bee_id = bee_detections.iloc[0][\"bee_id\"]\n",
    "        x_arr = bee_detections[\"x_pixels\"].to_numpy()\n",
    "        y_arr = bee_detections[\"y_pixels\"].to_numpy()\n",
    "        timestamps = bee_detections[\"timestamp\"].to_numpy()\n",
    "        orientations = bee_detections[\"orientation_hive\"].to_numpy()\n",
    "\n",
    "        orient_errors = []\n",
    "        row_cnt = len(bee_detections)\n",
    "\n",
    "        for i in range(row_cnt - next_detections_to_check_cnt - 1):\n",
    "            orientation_error = get_orientation_error_for_detection(i, x_arr, y_arr, timestamps, orientations, next_detections_to_check_cnt)\n",
    "            if orientation_error is not None:\n",
    "                orient_errors.append(orientation_error)\n",
    "\n",
    "        final_orient_error = 0\n",
    "        if len(orient_errors) > min_required_errors:\n",
    "            cutoff = math.floor(len(orient_errors) * 0.45)\n",
    "            orient_errors.sort()\n",
    "            orient_errors = orient_errors[cutoff:-cutoff]\n",
    "            final_orient_error = sum(orient_errors) / len(orient_errors)\n",
    "        orientation_error_by_bee[bee_id] = final_orient_error\n",
    "    return orientation_error_by_bee\n",
    "\n",
    "\n",
    "def apply_orientation_errors_to_dataframe(\n",
    "    orientation_error_by_bee: dict\n",
    "):\n",
    "    \"\"\"Applies the orientation errors by bee to all detections for the corresponing bees\n",
    "\n",
    "    Args:\n",
    "        orientation_error_by_bee (dict): Dictionary with bee-IDs as keys and orientation errors as values\n",
    "    \"\"\"\n",
    "    for bee_id, orientation_error in orientation_error_by_bee.items():\n",
    "        frame_id_df['orientation_hive'] = frame_id_df['orientation_hive'].mask(\n",
    "                                              frame_id_df['bee_id'] == bee_id, \n",
    "                                              frame_id_df['orientation_hive'] + orientation_error\n",
    "                                          )\n",
    "    frame_id_df.loc[frame_id_df[\"orientation_hive\"] < 0, \"orientation_hive\"] += 2 * np.pi\n",
    "    frame_id_df.loc[frame_id_df[\"orientation_hive\"] > 2 * np.pi, \"orientation_hive\"] -= 2 * np.pi\n",
    "\n",
    "\n",
    "def save_orientation_errors_to_csv(\n",
    "    orientation_error_by_bee: dict, \n",
    "    orient_errors_path: str\n",
    "):\n",
    "    \"\"\"Saves orientation errors as CSV-file\n",
    "\n",
    "    Args:\n",
    "        orientation_error_by_bee (dict): Dictionary with bee-IDs as keys and orientation errors as values\n",
    "        orient_errors_path (str): Path to CSV-file\n",
    "    \"\"\"\n",
    "    df_orientation_errors = pd.DataFrame(orientation_error_by_bee.items(), columns=['bee_id', 'orientation_error'])\n",
    "    df_orientation_errors.to_csv(orient_errors_path, sep='\\t', index=False, header=True)\n",
    "    logger.info(\"Saved orientation errors\")\n",
    "\n",
    "\n",
    "\n",
    "orientation_error_by_bee = {}\n",
    "\n",
    "# load/calculated orientation errors\n",
    "if os.path.exists(ORIENT_ERRORS_FILE) and not OVERWRITE_ORIENT_ERRORS_IF_EXIST:\n",
    "    df_orientation_errors = pd.read_csv(ORIENT_ERRORS_FILE, sep='\\t', index_col=0) \n",
    "    orientation_error_by_bee = df_orientation_errors['orientation_error'].to_dict()\n",
    "    logger.info(\"Imported orientation errors.\")\n",
    "else:\n",
    "    if not os.path.exists(ORIENT_ERRORS_FILE):\n",
    "        logger.warning(\"No orientation error file found.\")\n",
    "    orientation_error_by_bee = get_all_orientation_errors()\n",
    "    save_orientation_errors_to_csv(orientation_error_by_bee, ORIENT_ERRORS_FILE)\n",
    "\n",
    "apply_orientation_errors_to_dataframe(orientation_error_by_bee)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8636833",
   "metadata": {},
   "source": [
    "# Get the cropped images for the bees with the corresponding tag masks and body masks (for other bees)\n",
    "This cell was adapted from https://github.com/nebw/unsupervised_behaviors/blob/master/unsupervised_behaviors/data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed50ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_mask_for_detections(\n",
    "    detections: pd.DataFrame,\n",
    "    video_manager: CustomVideoManager,\n",
    "    image_size_px: int = 320,\n",
    "    image_crop_px: int = 48,\n",
    "    tag_mask_size_px: int = 20,\n",
    "    body_center_offset_px: int = 40,\n",
    "    body_mask_length_px: int = 120,\n",
    "    body_mask_width_px: int = 80,\n",
    "    egocentric: bool = True,\n",
    "    use_clahe: bool = True,\n",
    "    clahe_kernel_size_px: int = 25,\n",
    "    n_jobs: int = -1,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"Fetch image regions from cached images of raw BeesBook videos centered on detections of tagged bees. \n",
    "       Automatically generate loss masks for ellipsoid region around bee based on body orientation and tags \n",
    "       of all individuals visible in the image region. Optionally automatically rotate image region according \n",
    "       to body orientation.\n",
    "\n",
    "    Args:\n",
    "        detections (pd.DataFrame): Dataframe with detections.\n",
    "        video_manager (CustomVideoManager): Manages cache.\n",
    "        image_size_px (int, optional): Image size before cropping. Defaults to 320.\n",
    "        image_crop_px (int, optional): Crop amount after rotation. Defaults to 48.\n",
    "        tag_mask_size_px (int, optional): Size of tag mask in pixels. Defaults to 20.\n",
    "        body_center_offset_px (int, optional): Offset from tag (bee coordinates) to body center. Defaults to 40.\n",
    "        body_mask_length_px (int, optional): Length of body mask. Defaults to 120.\n",
    "        body_mask_width_px (int, optional): Width of body mask. Defaults to 80.\n",
    "        egocentric (bool, optional): Whether to rotate the frames so that the focus bee is egocentrically aligned. \n",
    "            Defaults to True.\n",
    "        use_clahe (bool, optional): Process entire frame using CLAHE. Defaults to True.\n",
    "        clahe_kernel_size_px (int, optional): Kernel size for CLAHE. Defaults to 25.\n",
    "        n_jobs (int, optional): Number of parallel jobs for processing. Defaults to -1.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, pd.DataFrame]: Images, tag masks, body masks and the dataframe adjusted \n",
    "            to the new crop region\n",
    "    \"\"\"\n",
    "\n",
    "    def rotate_crop(image: np.array, rotation_deg: float) -> np.array:\n",
    "        image = skimage.transform.rotate(image, rotation_deg)\n",
    "        image = image[image_crop_px:-image_crop_px, image_crop_px:-image_crop_px]\n",
    "        return image\n",
    "\n",
    "\n",
    "    def get_tag_mask(frame, all_detections_df):\n",
    "        tag_mask = np.ones_like(frame)\n",
    "        for _, row in all_detections_df.iterrows():\n",
    "            tag_mask[\n",
    "                skimage.draw.disk((row.y_pixels, row.x_pixels), tag_mask_size_px, shape=frame.shape)\n",
    "            ] = 0\n",
    "        return tag_mask\n",
    "\n",
    "\n",
    "    def extract_images_from_frame(\n",
    "        frame_detections: pd.DataFrame, \n",
    "        frame_path: str,\n",
    "        egocentric: bool = True,\n",
    "    ) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "        images = []\n",
    "        tag_masks = []\n",
    "        body_masks = []\n",
    "        rows = []\n",
    "\n",
    "        assert frame_detections.frame_id.nunique() == 1\n",
    "\n",
    "        frame = imageio.v3.imread(frame_path, plugin=\"opencv\", colorspace=\"GRAY\")\n",
    "        if use_clahe:\n",
    "            frame = skimage.exposure.equalize_adapthist(frame, kernel_size=(clahe_kernel_size_px, clahe_kernel_size_px))\n",
    "\n",
    "        tag_mask = get_tag_mask(frame, frame_detections)\n",
    "\n",
    "        for _, row in frame_detections.iterrows():\n",
    "            center_x = row.x_pixels - np.cos(row.orientation_hive) * body_center_offset_px\n",
    "            center_y = row.y_pixels - np.sin(row.orientation_hive) * body_center_offset_px\n",
    "\n",
    "            # assert center point always within frame (even after trajectory extrapolation)\n",
    "            center_x = max(0, center_x)\n",
    "            center_x = min(frame.shape[1] - 1, center_x)\n",
    "            center_y = max(0, center_y)\n",
    "            center_y = min(frame.shape[0] - 1, center_y)\n",
    "\n",
    "            center = np.array((center_x, center_y))\n",
    "\n",
    "            if egocentric:\n",
    "                # so that bee is facing to the right (row.orientation_hive + np.pi / 2 for facing upwards)\n",
    "                rotation_deg = (1 / (2 * np.pi)) * 360 * row.orientation_hive\n",
    "            else:\n",
    "                rotation_deg = 0\n",
    "\n",
    "            image = bb_behavior.utils.images.get_crop_from_image(\n",
    "                center, frame, width=image_size_px, clahe=False\n",
    "            )\n",
    "            image = (rotate_crop(image, rotation_deg) * 255).astype(np.uint8)\n",
    "\n",
    "            body_mask = np.zeros_like(frame)\n",
    "            body_coords = skimage.draw.ellipse(\n",
    "                center[1],\n",
    "                center[0],\n",
    "                body_mask_length_px,\n",
    "                body_mask_width_px,\n",
    "                rotation=-(row.orientation_hive - np.pi / 2),\n",
    "                shape=frame.shape,\n",
    "            )\n",
    "            body_mask[body_coords] = 1\n",
    "            body_mask = (\n",
    "                bb_behavior.utils.images.get_crop_from_image(\n",
    "                    center, body_mask, width=image_size_px, clahe=False\n",
    "                )\n",
    "                == 255\n",
    "            )\n",
    "            body_mask = rotate_crop(body_mask, rotation_deg) > 0.5\n",
    "            \n",
    "            task_mask = (\n",
    "                bb_behavior.utils.images.get_crop_from_image(\n",
    "                    center, tag_mask, width=image_size_px, clahe=False\n",
    "                )\n",
    "                == 255\n",
    "            )\n",
    "            task_mask = rotate_crop(task_mask, rotation_deg) > 0.5\n",
    "\n",
    "            \n",
    "            images.append(image)\n",
    "            body_masks.append(body_mask)\n",
    "            tag_masks.append(task_mask)\n",
    "            rows.append(row.values)\n",
    "\n",
    "        return images, tag_masks, body_masks, rows\n",
    "\n",
    "    if len(detections.index) == 0:\n",
    "        return\n",
    "\n",
    "    images = []\n",
    "    tag_masks = []\n",
    "    body_masks = []\n",
    "    rows = []\n",
    "\n",
    "    logger.debug(f'Detection count in video: {len(detections.index)}')\n",
    "    \n",
    "    detections_by_frame = detections.groupby(\"frame_id\")\n",
    "\n",
    "    # preload file paths because video_manager can't be used in parallel.\n",
    "    frame_paths = []\n",
    "    for _, frame_detections in detections_by_frame:\n",
    "        frame_paths.append(video_manager.get_frame_id_path(frame_detections.frame_id.iloc[0]))\n",
    "    \n",
    "    logger.debug(f\"Processing {len(frame_paths)} cached images ...\")\n",
    "    parallel = joblib.Parallel(prefer=\"processes\", n_jobs=n_jobs)(\n",
    "        joblib.delayed(extract_images_from_frame)(\n",
    "            frame_detections, \n",
    "            frame_path, \n",
    "            egocentric=egocentric\n",
    "        )\n",
    "        for (_, frame_detections), frame_path in zip(detections_by_frame, frame_paths)\n",
    "    )\n",
    "\n",
    "    logger.debug(\"Processing of cached images complete.\")\n",
    "\n",
    "    for results in parallel:\n",
    "        images += results[0]\n",
    "        tag_masks += results[1]\n",
    "        body_masks += results[2]\n",
    "        rows += results[3]\n",
    "\n",
    "    images = np.stack(images)\n",
    "    tag_masks = np.stack(tag_masks)\n",
    "    body_masks = np.stack(body_masks)\n",
    "    detections = pd.DataFrame(np.stack(rows), columns=detections.columns)\n",
    "\n",
    "    return images, tag_masks, body_masks, detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f8279",
   "metadata": {},
   "source": [
    "# Cache frames, extract and process them and write to videos\n",
    "Main tasks:\n",
    "1. Cache all frames with bees in them from the videos. \n",
    "2. From the cached frames extract all cropped and rotated frames showing the individual bees in the frames.\n",
    "3. Apply the masks and write the extracted frames to videos for each individual bee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_continuous_ranges(arr: np.ndarray) -> list:\n",
    "    \"\"\"Finds starts and ends of continuous ranges (no value gaps) in array\n",
    "\n",
    "    Args:\n",
    "        arr (np.ndarray): Input array\n",
    "\n",
    "    Returns:\n",
    "        list: List of Tuples containing the start and end index of each continuous range in arr\n",
    "    \"\"\"\n",
    "    diffs = np.diff(arr)\n",
    "\n",
    "    # Indices where a gap occurs\n",
    "    gap_indices = np.where(diffs > 1)[0]\n",
    "    \n",
    "    # Start and end of continuous segments\n",
    "    starts = np.concatenate(([0], gap_indices + 1))\n",
    "    ends = np.concatenate((gap_indices + 1, [len(arr)]))\n",
    "\n",
    "    return list(zip(starts, ends))\n",
    "\n",
    "\n",
    "def extract_all_videos(\n",
    "    frame_id_df: pd.DataFrame, \n",
    "    min_frames: int = 30\n",
    "):\n",
    "    \"\"\"Extracts cropped videos of bees with enough consecutive frame detections\n",
    "\n",
    "    Args:\n",
    "        frame_id_df (pd.DataFrame): Dataframe with detections\n",
    "        min_frames (int, optional): Minimum number of consecutive frames required to generate a video. \n",
    "            Defaults to 30.\n",
    "    \"\"\"\n",
    "\n",
    "    df_grouped = frame_id_df.groupby('video_filename', sort=False)\n",
    "    for video_filename, df_video in df_grouped:\n",
    "        logger.info(f\"Processing video {video_filename} ...\")\n",
    "\n",
    "        video_manager.cache_frames(\n",
    "            frame_ids=np.sort(df_video['frame_id'].unique()), \n",
    "            video_name=video_filename, \n",
    "            frame_indices=np.sort(df_video['frame_index'].unique())\n",
    "        )\n",
    "\n",
    "        # get images and masks for all detections in one video\n",
    "        images, tag_masks, body_masks, df_video_after = get_image_and_mask_for_detections(\n",
    "            detections=df_video, \n",
    "            video_manager=video_manager, \n",
    "            n_jobs=N_JOBS\n",
    "        )\n",
    "\n",
    "        # apply the masks\n",
    "        images_masked = images * tag_masks\n",
    "        \n",
    "        if APPLY_BODY_MASKS:\n",
    "            images_masked *= body_masks\n",
    "\n",
    "        # sort the df by bee_id and timestamp and apply the same sorting to the images\n",
    "        df_video_sorted = df_video_after.sort_values(['bee_id', 'timestamp'])\n",
    "        sorting_indices = df_video_sorted.index\n",
    "        images_sorted = [images_masked[i] for i in sorting_indices]\n",
    "\n",
    "        df_video_grouped = df_video_sorted.groupby('bee_id', sort=False)\n",
    "        # dict{bee_id -> indices}\n",
    "        df_video_grouped_indices = df_video_grouped.indices\n",
    "        df_video_grouped_indices_iter = iter(df_video_grouped_indices)\n",
    "        \n",
    "        for bee_id, df_bee_video in df_video_grouped:\n",
    "            # there can be frame gaps in the data, where the bee was not detected\n",
    "            frame_indices = df_bee_video[\"frame_index\"].to_numpy()\n",
    "            ranges = find_continuous_ranges(frame_indices)\n",
    "\n",
    "            cam_str = \"cam-\" + str(df_bee_video.cam_id.iloc[0])\n",
    "            indices_by_bee_id = df_video_grouped_indices.get(next(df_video_grouped_indices_iter))\n",
    "\n",
    "            # for every continous series of frames for this bee\n",
    "            for (start, stop) in ranges:\n",
    "                frame_cnt = stop - start\n",
    "                start_time = df_bee_video.timestamp.iloc[start].strftime('%Y-%m-%dT%H.%M.%S')\n",
    "                end_time = df_bee_video.timestamp.iloc[stop-1].strftime('%Y-%m-%dT%H.%M.%S')\n",
    "                filename = \"_\".join((cam_str, str(bee_id), (start_time + \"--\" + end_time), str(frame_cnt) + \"frames\" + \".mp4\"))\n",
    "                images_bee = [images_sorted[i] for i in indices_by_bee_id[start:stop]]\n",
    "                if len(images_bee) < min_frames:\n",
    "                    continue\n",
    "                video_manager.write_to_video(images=images_bee, filename=filename, frame_rate=FRAME_RATE)\n",
    "        video_manager.clear_video_cache()\n",
    "    \n",
    "    \n",
    "extract_all_videos(frame_id_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
