{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d38bd2",
   "metadata": {},
   "source": [
    "# Notebook to generate video clips of potential trophallaxis from Konstanz dataset (scented bee petri dish videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ecd720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import bb_behavior.utils.images\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import imageio\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import glob\n",
    "import logging\n",
    "import random\n",
    "import json\n",
    "from scipy.ndimage import binary_closing, binary_opening, binary_dilation\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import iqr\n",
    "from video_utils import CustomVideoManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1550d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='troph_video_gen_konst_data.log',\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# write info to console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(logging.Formatter(\n",
    "    '%(levelname)s - %(message)s'\n",
    "))\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Starting video generation ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e821dd",
   "metadata": {},
   "source": [
    "## Set global variables\n",
    "\n",
    "| Variable | Explanation |\n",
    "| --- | --- |\n",
    "| GENERATE_VIDEOS | whether to generate video clips of potential troph. events; if False then only the metadata for the videos is generated (much faster) |\n",
    "| USE_ZOOM_LEVELS | whether to generate the clips in different zoom levels (for each video 1 of 3 zoom levels is chosen randomly) |\n",
    "| USE_CROP_CENTER_SHIFT | whether to generate the clips with a random crop center shift for the clips (so that troph. is not always in the center) |\n",
    "| USE_EGOCENTRIC_ALIGNMENT | whether to egocentrically align the videos to the focus bee (focus bee is always the bee with the lower bee ID and always mentioned first in the generated troph. clip name) |\n",
    "| DEFAULT_CM_PER_PX | All values in pixels in this notebook get adjusted to the video dimensions because they are in relation to 1920x1080 videos (which all have a cm_per_px of around 0.0136). They get scaled by multiplying with this value and diving by the video-specific cm_per_pixel that was used in TRex. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_VIDEOS = True\n",
    "USE_ZOOM_LEVELS = False\n",
    "USE_CROP_CENTER_SHIFT = False\n",
    "USE_EGOCENTRIC_ALIGNMENT = True\n",
    "\n",
    "# only use crop center shift OR egocentric alignment, not both!\n",
    "assert USE_CROP_CENTER_SHIFT == False or USE_EGOCENTRIC_ALIGNMENT == False\n",
    "\n",
    "VIDEO_ROOT = '/mnt/windows-ssd/BeesData/scented_bees_video_files/'\n",
    "CACHE_PATH = '/mnt/windows-ssd/BeesData/tmp/'\n",
    "POSTURE_FILES_PATH = '/mnt/windows-ssd/BeesData/scented_bees_posture_files/'\n",
    "VIDEO_OUTOUT_PATH = '/mnt/windows-ssd/BeesData/scented_bees_troph_clips/'\n",
    "METADATA_OUTPUT_FILENAME = f\"metadata_troph_konstanz_videos_{datetime.today().strftime('%Y-%m-%d')}.json\"\n",
    "\n",
    "FRAME_RATE = 50\n",
    "FRAMES_PER_MINUTE = 60 * 50\n",
    "# cut off the first five minutes of frames because bees are still waking up\n",
    "MINUTES_CUTOFF_START = 5\n",
    "FRAME_CUTOFF_START = MINUTES_CUTOFF_START * FRAMES_PER_MINUTE\n",
    "\n",
    "DEFAULT_CM_PER_PX = 0.0136\n",
    "\n",
    "BODY_TO_TROPHALLAXIS_CENTER_OFFSET_PX = 22\n",
    "GROUPS = [\"hex_*.npz\", \"OLE_*.npz\", \"OCI_*.npz\"]\n",
    "COLS_BEE_DATA = ['x_pixels', 'y_pixels', 'orientation']\n",
    "\n",
    "#############################################################################################\n",
    "# wcentroid is based on centroid weighted by pixel values, \n",
    "# pcentroid is based on posture centroid (the center of the midline)\n",
    "# centroid is center of mass of all thresholded pixels\n",
    "# nothing after the hashtag means based on head position\n",
    "#############################################################################################\n",
    "COLS_TO_EXCLUDE = [\n",
    "    'tracklets','tracklet_vxys','video_size', 'id', 'frame_rate', \n",
    "    'ACCELERATION#pcentroid', 'ACCELERATION#wcentroid', 'ANGULAR_A#centroid', \n",
    "    'ACCELERATION#wcentroid', 'ANGULAR_V#centroid', 'BORDER_DISTANCE#pcentroid', \n",
    "    'AX', 'AY', 'MIDLINE_OFFSET',\n",
    "    'poseX0', 'poseY0', 'poseX1', 'poseY1', 'poseX2', 'poseY2',\n",
    "    'poseX3', 'poseY3', 'poseX4', 'poseY4', 'poseX5', 'poseY5',\n",
    "    'SPEED#wcentroid', 'SPEED#pcentroid', 'SPEED', 'VX', 'VY', 'X#wcentroid', \n",
    "    'Y#wcentroid', 'midline_length', 'midline_segment_length', 'midline_x', \n",
    "    'midline_y', 'missing', 'normalized_midline', 'num_pixels', 'timestamp'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca645c1",
   "metadata": {},
   "source": [
    "## Build the dataframe from tracking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b80900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe() -> pd.DataFrame:\n",
    "    frame_id_offset = 0\n",
    "    files_to_grab = []\n",
    "    if GROUPS is not None and len(GROUPS) > 0:\n",
    "        for group in GROUPS:\n",
    "            files_to_grab.extend(glob.glob(POSTURE_FILES_PATH + group))\n",
    "        posture_file_names = sorted(files_to_grab)\n",
    "    else:\n",
    "        posture_file_names = sorted(glob.glob(POSTURE_FILES_PATH + \"*.npz\"))\n",
    "\n",
    "    for file_idx, file in enumerate(posture_file_names):\n",
    "        try:\n",
    "            data = np.load(file)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"There was a problem loading the data file {file}. Exception: {e}\")\n",
    "            raise Exception(e)\n",
    "\n",
    "        keys = data.files\n",
    "\n",
    "        # Figure out how many rows (use 'time' as canonical-but any 1D key works)\n",
    "        n = data['time'].shape[0]\n",
    "\n",
    "        columns = {}\n",
    "        for k in keys:\n",
    "            if k in COLS_TO_EXCLUDE:\n",
    "                continue\n",
    "            v = data[k]\n",
    "            \n",
    "            # Scalar → broadcast to length n\n",
    "            if np.ndim(v) == 0 or k == 'cm_per_pixel':\n",
    "                columns[k] = np.repeat(v.item(), n)\n",
    "            # 1-D array (including object-dtype) → straight in\n",
    "            elif v.ndim == 1:\n",
    "                columns[k] = v\n",
    "            # Multi-D numeric array → flatten trailing dims into separate columns\n",
    "            else:\n",
    "                flat = v.reshape(n, -1)\n",
    "                for i in range(flat.shape[1]):\n",
    "                    columns[f\"{k}_{i}\"] = flat[:, i]\n",
    "\n",
    "        # Build the DataFrame\n",
    "        df_file = pd.DataFrame(columns)\n",
    "        data_filename = (file.split(\"/\")[-1]).split(\".\")[0]\n",
    "        df_file['data_filename'] = data_filename\n",
    "\n",
    "        # X and Y are in cm (convert to px)\n",
    "        df_file = df_file.rename(columns={'cm_per_pixel': 'cm_per_px'})\n",
    "        cm_per_px = df_file['cm_per_px'].iloc[0]\n",
    "        df_file['x_pixels'] = np.round(df_file['X'] / cm_per_px)\n",
    "        df_file['y_pixels'] = np.round(df_file['Y'] / cm_per_px)\n",
    "\n",
    "        df_file['bee_id'] = data['id'][0]\n",
    "        df_file['frame_index'] = df_file['frame'].astype(int)\n",
    "        df_file = df_file.drop(columns=['frame', 'X', 'Y'])\n",
    "        df_file = df_file.rename(columns={'ANGLE': 'orientation'})\n",
    "\n",
    "        df_file = df_file[df_file.frame_index >= FRAME_CUTOFF_START]\n",
    "        \n",
    "        frames_cnt = len(df_file['frame_index'])\n",
    "        df_file['frame_id'] = df_file['frame_index'] + frame_id_offset\n",
    "        if (file_idx+1) % 4 == 0:\n",
    "            frame_id_offset += frames_cnt\n",
    "\n",
    "        df_files.append(df_file)\n",
    "\n",
    "    df = pd.concat(df_files)\n",
    "    df['video_filename'] = VIDEO_ROOT + df['data_filename'].str.split('_fish').str[0] + \".mp4\"\n",
    "    return df\n",
    "\n",
    "df_files = []\n",
    "df = build_dataframe()\n",
    "\n",
    "print(df.keys())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba5916",
   "metadata": {},
   "source": [
    "## Combine the tracking dataframe with the video file dataframe\n",
    "\n",
    "The Custom video manager is used for caching and extracting frames from videos as well as saving frames to videos. It is partly adapted from a notebook by Jacob Davidson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10efaccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_manager = CustomVideoManager(VIDEO_ROOT, CACHE_PATH, VIDEO_OUTOUT_PATH, max_workers=16)\n",
    "video_manager.clear_video_cache()\n",
    "\n",
    "videos_df = video_manager.get_all_video_files()\n",
    "df = pd.merge(df, videos_df, on='video_filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfab222",
   "metadata": {},
   "source": [
    "## Get the cropped images for the bees\n",
    "This cell was adapted from https://github.com/nebw/unsupervised_behaviors/blob/master/unsupervised_behaviors/data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c22a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frames(\n",
    "    detections: pd.DataFrame,\n",
    "    video_manager: CustomVideoManager,\n",
    "    px_adj_ratio: float,\n",
    "    uncropped_image_size_px: int = 120,\n",
    "    image_crop_px: int = 20,\n",
    "    egocentric: bool = True,\n",
    "    generate_videos: bool = True,\n",
    "    use_zoom_levels: bool = False,\n",
    "    use_crop_center_shift: bool = False,\n",
    "    use_clahe: bool = True,\n",
    "    clahe_kernel_size_px: int = 25,\n",
    "    n_jobs: int = -1,\n",
    ") -> Tuple[np.ndarray, dict]:\n",
    "    \"\"\"Create a metadata dictionary for this potential trophallaxis run. Get cached images, crop them to the area \n",
    "       of interest and optionally rotate image region in each image so that the focus bee is egocentrically aligned.\n",
    "\n",
    "    Args:\n",
    "        detections (pd.DataFrame): Dataframe with detections.\n",
    "        video_manager (CustomVideoManager): Manages cache.\n",
    "        px_adj_ratio (float): Ratio for pixel adjustment in this video compared to 1920x1080 videos\n",
    "        uncropped_image_size_px (int, optional): Image size before cropping. Defaults to 120.\n",
    "        image_crop_px (int, optional): Crop amount after rotation. Defaults to 20.\n",
    "        egocentric (bool, optional): Whether to rotate the frames so that the focus bee is egocentrically aligned. \n",
    "            Defaults to True.\n",
    "        generate_videos (bool, optional): Whether to generate video clips. Defaults to True.\n",
    "        use_zoom_levels (bool, optional): Whether to generate the clips with zoom levels. Defaults to False.\n",
    "        use_crop_center_shift (bool, optional): Whether to generate the clips with a random crop center shift. \n",
    "            Defaults to False.\n",
    "        use_clahe (bool, optional): Process entire frame using CLAHE. Defaults to True.\n",
    "        clahe_kernel_size_px (int, optional): Kernel size for CLAHE. Defaults to 25.\n",
    "        n_jobs (int, optional): Number of parallel jobs for processing. Defaults to -1.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, dict]: Extracted image regions and a dictionary with metadata about the video clip\n",
    "    \"\"\"\n",
    "\n",
    "    def rotate_crop_img(image: np.ndarray, rotation_deg: float, image_crop_px: int) -> np.ndarray:\n",
    "        image = skimage.transform.rotate(image, rotation_deg)\n",
    "        image = image[image_crop_px:-image_crop_px, image_crop_px:-image_crop_px]\n",
    "        return image\n",
    "\n",
    "    \n",
    "    def extract_crop_from_frame(\n",
    "        detection: pd.DataFrame, \n",
    "        frame_path: str,\n",
    "        uncropped_image_size_px: int,\n",
    "        image_crop_px: int,\n",
    "        x_shift_px: int,\n",
    "        y_shift_px: int,\n",
    "        fetch_image: bool = True\n",
    "    ) -> np.ndarray:\n",
    "        row = detection.iloc[0]\n",
    "        if fetch_image:\n",
    "            frame = imageio.v3.imread(frame_path, plugin=\"opencv\", colorspace=\"GRAY\")\n",
    "                \n",
    "            if use_clahe:\n",
    "                frame = skimage.exposure.equalize_adapthist(frame, kernel_size=(clahe_kernel_size_px, clahe_kernel_size_px))\n",
    "\n",
    "            if egocentric:\n",
    "                # bee will be facing to the right (row.orientation_0 + np.pi / 2 would be facing upwards)\n",
    "                rotation_deg = (1 / (2 * np.pi)) * 360 * row.orientation_0\n",
    "            else:\n",
    "                rotation_deg = 0\n",
    "            \n",
    "            # ensure that center is in frame\n",
    "            min_px_to_edge = uncropped_image_size_px // 2\n",
    "            center_x_shifted = max(min(row.crop_area_center_x + x_shift_px, frame.shape[1] - min_px_to_edge), min_px_to_edge)\n",
    "            center_y_shifted = max(min(row.crop_area_center_y + y_shift_px, frame.shape[0] - min_px_to_edge), min_px_to_edge)\n",
    "\n",
    "            center = np.array((center_x_shifted, center_y_shifted))\n",
    "\n",
    "            image = bb_behavior.utils.images.get_crop_from_image(\n",
    "                center, frame, width=uncropped_image_size_px, clahe=False\n",
    "            )\n",
    "            image = (rotate_crop_img(image, rotation_deg, image_crop_px) * 255).astype(np.uint8)\n",
    "        return image\n",
    "    \n",
    "\n",
    "\n",
    "    logger.debug(f'Detection count in video: {len(detections.index)}')\n",
    "    if len(detections.index) == 0:\n",
    "        return\n",
    "\n",
    "    # adjust pixel values to video dimensions\n",
    "    uncropped_image_size_px = math.ceil(uncropped_image_size_px * px_adj_ratio)\n",
    "    image_crop_px = math.floor(image_crop_px * px_adj_ratio)\n",
    "    cropped_image_size_px = uncropped_image_size_px - 2 * image_crop_px\n",
    "    if use_zoom_levels:\n",
    "        cropped_image_size_px = random.choice([\n",
    "            cropped_image_size_px,\n",
    "            math.floor(cropped_image_size_px * 1.5),\n",
    "            math.floor(cropped_image_size_px * 2)\n",
    "        ])\n",
    "        uncropped_image_size_px = math.ceil(cropped_image_size_px * math.sqrt(2))\n",
    "        uncropped_image_size_px = uncropped_image_size_px if uncropped_image_size_px % 2 == 0 else uncropped_image_size_px + 1\n",
    "        image_crop_px = int((uncropped_image_size_px - cropped_image_size_px) / 2)\n",
    "    \n",
    "\n",
    "    x_shift_px = 0\n",
    "    y_shift_px = 0\n",
    "    if use_crop_center_shift:\n",
    "        if egocentric:\n",
    "            raise ValueError(\"Only use egocentric if crop center shift is turned off!\")\n",
    "        # trophallaxis has to remain in the crop area\n",
    "        x_shift_px = random.randint(-(cropped_image_size_px // 3), \n",
    "                                    cropped_image_size_px // 3)\n",
    "        y_shift_px = random.randint(-(cropped_image_size_px // 3), \n",
    "                                    cropped_image_size_px // 3)\n",
    "        \n",
    "\n",
    "    detections_by_frame = detections.groupby(\"frame_id\")\n",
    "    # preload file paths because video_manager can't be used in parallel.\n",
    "    frame_paths = [video_manager.get_frame_id_path(frame_id) for frame_id in detections[\"frame_id\"].to_numpy()]\n",
    "    logger.info(f\"Processing {len(frame_paths)} cached images ...\")\n",
    "    parallel = joblib.Parallel(prefer=\"processes\", n_jobs=n_jobs)(\n",
    "        joblib.delayed(extract_crop_from_frame)(\n",
    "            frame_detection, \n",
    "            frame_path,\n",
    "            uncropped_image_size_px,\n",
    "            image_crop_px,\n",
    "            x_shift_px,\n",
    "            y_shift_px,\n",
    "            generate_videos\n",
    "        )\n",
    "        for (_, frame_detection), frame_path in zip(detections_by_frame, frame_paths)\n",
    "    )\n",
    "    logger.info(\"Processing of cached images complete.\")\n",
    "\n",
    "    images = []\n",
    "    if generate_videos:\n",
    "        for result in parallel:\n",
    "            images.append(result)\n",
    "    images = np.stack(images) if len(images) > 0 else np.array([])\n",
    "    metadata_value_dict = {\n",
    "        \"Trophallaxis video dimensions in px\": cropped_image_size_px,\n",
    "        \"x_shift_px\": x_shift_px,\n",
    "        \"y_shift_px\": y_shift_px,\n",
    "        \"start_frame_index\": detections.frame_index.iat[0].item(),\n",
    "        \"end_frame_index\": detections.frame_index.iat[-1].item(),\n",
    "    }\n",
    "    return images, metadata_value_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1854aba",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081be670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_missing_vals(\n",
    "    df_video: pd.DataFrame, \n",
    "    cols_to_interpol: list\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Interpolates inf values in the trajectory data\n",
    "\n",
    "    Args:\n",
    "        df_video (pd.DataFrame): dataframe, in which to interpolate\n",
    "        cols_to_interpol (list): columns to interpolate\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Interpolated dataframe\n",
    "    \"\"\"\n",
    "    df_video[cols_to_interpol] = (\n",
    "        df_video\n",
    "        .groupby('bee_id', sort=False)[cols_to_interpol]\n",
    "        # transform applies methods in-place\n",
    "        .transform(lambda x: x.replace(np.inf, np.nan)\n",
    "                              .interpolate(method='linear')\n",
    "                              .bfill()\n",
    "                              .ffill()))\n",
    "    return df_video\n",
    "\n",
    "\n",
    "def check_troph_conditions(\n",
    "    df_troph: pd.DataFrame, \n",
    "    px_adj_ratio: float, \n",
    "    shift_dist: float = 15.0, \n",
    "    max_dist: float = 40.0,\n",
    "    max_inv_orientation_diff_deg: float = 80.0\n",
    ") -> list[bool]:\n",
    "    \"\"\"Checks for each row (frame) of the dataframe if trophallaxis conditions for distance and relative orientation are fulfilled. \n",
    "\n",
    "    Args:\n",
    "        df_troph (pd.DataFrame): Dataframe\n",
    "        px_adj_ratio (float): Ratio for pixel adjustment in this video compared to 1920x1080 videos\n",
    "        shift_dist (float, optional): Shift in pixels in the orientation direction of the bee before distance calculation. \n",
    "            Defaults to 15.0.\n",
    "        max_dist (float, optional): Maximum allowed distance for trophallaxis condition to be fulfilled. Defaults to 40.0.\n",
    "        max_inv_orientation_diff_deg (float, optional): Maximum orientation difference in degrees between the bees if one \n",
    "            bee were rotated by 180 degrees. Defaults to 80.0.\n",
    "\n",
    "    Returns:\n",
    "        list[bool]: List of booleans with one field per frame whether the conditions are fulfilled\n",
    "    \"\"\"\n",
    "    shift_dist *= px_adj_ratio\n",
    "    max_dist *= px_adj_ratio\n",
    "\n",
    "    orient_0 = df_troph.orientation_0.to_numpy()\n",
    "    orient_1 = df_troph.orientation_1.to_numpy()\n",
    "    \n",
    "    orient_diff = np.abs(orient_0 - orient_1)\n",
    "    orient_diff = np.minimum(orient_diff, np.abs(2*np.pi - orient_diff))\n",
    "\n",
    "    x0_shifted = df_troph.x_pixels_0.to_numpy() + np.cos(orient_0) * shift_dist\n",
    "    x1_shifted = df_troph.x_pixels_1.to_numpy() + np.cos(orient_1) * shift_dist\n",
    "    y0_shifted = df_troph.y_pixels_0.to_numpy() + np.sin(orient_0) * shift_dist\n",
    "    y1_shifted = df_troph.y_pixels_1.to_numpy() + np.sin(orient_1) * shift_dist\n",
    "\n",
    "    dist = np.hypot(x0_shifted - x1_shifted, \n",
    "                    y0_shifted - y1_shifted) \n",
    "\n",
    "    # if the two bees have not more than 80 deg difference in orientation when one of the bees is rotated by pi (180 deg)\n",
    "    return (np.pi - orient_diff < np.deg2rad(max_inv_orientation_diff_deg)) & (dist < max_dist)\n",
    "\n",
    "\n",
    "def remove_bool_islands(\n",
    "    arr: np.ndarray, \n",
    "    structure_length: int = 25\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Removes boolean islands in the array with morphological functions\n",
    "\n",
    "    Args:\n",
    "        arr (np.ndarray): Boolean array\n",
    "        structure_length (int, optional): Length of structure of 1's used by scipy morph. functions. Defaults to 25.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Processed boolean array\n",
    "    \"\"\"\n",
    "    structure = np.ones(structure_length, dtype=bool)\n",
    "    # closing first because removing false islands in true regions takes priority\n",
    "    return binary_opening(binary_closing(arr, structure=structure), structure=structure)\n",
    "\n",
    "\n",
    "def find_long_true_runs(\n",
    "    arr: np.ndarray, \n",
    "    min_run: int = 250, \n",
    "    n_frame_padding: int = 10\n",
    ") -> Tuple[np.ndarray, list[int]]:\n",
    "    \"\"\"Finds true regions with a certain minimum length in a boolean array\n",
    "\n",
    "    Args:\n",
    "        arr (np.ndarray): Boolean array\n",
    "        min_run (int, optional): Minimum run length. Defaults to 250.\n",
    "        n_frame_padding (int, optional): Number of frames to pad each true run before start and after end. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, list[int]]: Same as arr but with only true runs of min. length min_run left \n",
    "            and a list of Tuples with start and end indices for each run\n",
    "    \"\"\"\n",
    "    meets_troph_conds = np.full(arr.shape, False)\n",
    "    edges = np.flatnonzero(np.diff(arr))\n",
    "\n",
    "    starts = np.r_[0, edges + 1]\n",
    "    ends = np.r_[edges, len(arr) - 1]\n",
    "\n",
    "    # contains all starts (of true runs and of false runs)\n",
    "    true_mask = arr[starts]\n",
    "    # contains all values in starts where true_mask is also true\n",
    "    true_starts = starts[true_mask]\n",
    "    true_ends = ends[true_mask]\n",
    "\n",
    "    lengths = true_ends - true_starts + 1\n",
    "\n",
    "    true_starts = true_starts[lengths >= min_run]\n",
    "    lengths = lengths[lengths >= min_run]\n",
    "\n",
    "    runs_frame_indices = []\n",
    "    for start, length in zip(true_starts, lengths):\n",
    "        # pad run\n",
    "        end = min(len(arr), start+length+n_frame_padding)\n",
    "        start = max(0, start-n_frame_padding)\n",
    "        meets_troph_conds[start:end] = True\n",
    "        # start and end are relative to arr\n",
    "        runs_frame_indices.append(range(start+FRAME_CUTOFF_START, end+FRAME_CUTOFF_START))\n",
    "    return meets_troph_conds, runs_frame_indices\n",
    "\n",
    "\n",
    "def smooth_column_data(\n",
    "    df: pd.DataFrame, \n",
    "    cols_to_smooth: list, \n",
    "    window_length: int = 5, \n",
    "    savgol_order: int = 3\n",
    "):\n",
    "    \"\"\"Smooths specific columns of the dataframe with a Savitzky-Golay filter\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dateframe\n",
    "        cols_to_smooth (list): Columns to smooth\n",
    "        window_length (int, optional): Window length of filter. Defaults to 5.\n",
    "        savgol_order (int, optional): Order of filter. Defaults to 3.\n",
    "    \"\"\"\n",
    "    df[cols_to_smooth] = savgol_filter(\n",
    "        df[cols_to_smooth].to_numpy(),\n",
    "        window_length=window_length,\n",
    "        polyorder=savgol_order,\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "def clean_orientation_outliers(\n",
    "    orientations: np.ndarray,\n",
    "    window_length: int = 11,\n",
    "    iqr_factor: float = 6.0,\n",
    "    dilation_iters: int = 3\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Clean orientation values by calculating a rolling median with a certain window length, taking the median of those values,\n",
    "       calculating the difference of each orientation value to this median, setting values to NaN where the difference is above \n",
    "       an IQR-based threshold, dilating those NaN regions and interpolating the NaN values.\n",
    "\n",
    "    Args:\n",
    "        orientations (np.ndarray): Orientation values in radians\n",
    "        window_length (int, optional): Length of rolling median window. Defaults to 11.\n",
    "        iqr_factor (float, optional): Used to calculate the threshold for outliers. Defaults to 6.0.\n",
    "        dilation_iters (int, optional): Binary dilation iterations. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Cleaned orientation array\n",
    "    \"\"\"\n",
    "\n",
    "    orientations = pd.Series(orientations)\n",
    "    median = orientations.rolling(window_length, center=True, min_periods=1).median()\n",
    "    deviation = np.abs(orientations - median)\n",
    "    deviation = np.minimum(deviation, np.abs(2*np.pi - deviation))\n",
    "\n",
    "    dev_iqr = iqr(deviation.dropna())\n",
    "\n",
    "    threshold = iqr_factor * dev_iqr\n",
    "    outlier_mask = deviation > threshold\n",
    "    outlier_mask = binary_dilation(outlier_mask.to_numpy(), iterations=dilation_iters)\n",
    "    orientations_masked = orientations.copy()\n",
    "    orientations_masked[outlier_mask] = np.nan\n",
    "\n",
    "    orientations_clean = (\n",
    "        pd.Series(orientations_masked)\n",
    "            .interpolate(method=\"linear\")\n",
    "            .bfill()\n",
    "            .ffill()\n",
    "            .to_numpy()\n",
    "    )\n",
    "    return orientations_clean\n",
    "\n",
    "\n",
    "def _clean_trajectory_outliers(\n",
    "    x: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    iqr_factor: float = 3.0, \n",
    "    dilation_iters: int = 3\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Clean trajectory outliers by calculating the z-score of x- and y-values, combining those,\n",
    "       calculating the IQR, setting values above iqr_factor * IQR to NaN, dilating those NaN regions\n",
    "       and interpolating them.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): X-values in pixels\n",
    "        y (np.ndarray): Y-values in pixels\n",
    "        iqr_factor (float, optional): Used to calculate the threshold for outliers. Defaults to 3.0.\n",
    "        dilation_iters (int, optional): Binary dilation iterations. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: The cleaned x- and y-values\n",
    "    \"\"\"\n",
    "\n",
    "    mean_x = np.nanmean(x)\n",
    "    mean_y = np.nanmean(y)\n",
    "    # Avoid division by zero: if std is 0, set to tiny value\n",
    "    eps = 1e-8\n",
    "    std_x = min(np.nanstd(x), eps)\n",
    "    std_y = min(np.nanstd(y), eps)\n",
    "\n",
    "    zx = (x - mean_x) / std_x\n",
    "    zy = (y - mean_y) / std_y\n",
    "\n",
    "    combined_z = np.hypot(zx, zy)\n",
    "\n",
    "    iqr_val = iqr(combined_z)\n",
    "\n",
    "    outlier_mask = combined_z > iqr_factor * iqr_val\n",
    "    outlier_mask = binary_dilation(outlier_mask, iterations=dilation_iters)\n",
    "\n",
    "    # mark both axes as NaN for outlier frames\n",
    "    x[outlier_mask] = np.nan\n",
    "    y[outlier_mask] = np.nan\n",
    "\n",
    "    x_clean = pd.Series(x).interpolate(method=\"linear\").bfill().ffill().to_numpy()\n",
    "    y_clean = pd.Series(y).interpolate(method=\"linear\").bfill().ffill().to_numpy()\n",
    "\n",
    "    return x_clean, y_clean\n",
    "\n",
    "\n",
    "def clean_trajectory_outliers(\n",
    "    df_troph_video: pd.DataFrame, \n",
    "    video_width: int, \n",
    "    video_height: int, \n",
    "    px_adj_ratio: float\n",
    "):\n",
    "    \"\"\"Clean the outliers in trajectory, which is used for crop center, by calculating and adjusting the crop center\n",
    "       and calling _clean_trajectory_outliers.\n",
    "\n",
    "    Args:\n",
    "        df_troph_video (pd.DataFrame): Dateframe for one potential trophallaxis event\n",
    "        video_width (int): Original video width in pixels\n",
    "        video_height (int): Original video height in pixels\n",
    "        px_adj_ratio (float): Ratio for pixel adjustment in this video compared to 1920x1080 videos\n",
    "    \"\"\"\n",
    "    orientation_focus_bee = df_troph_video[\"orientation_0\"].to_numpy()\n",
    "\n",
    "    # use orientation of bee 0 for center calculation\n",
    "    body_center_adj_x = np.cos(orientation_focus_bee) * BODY_TO_TROPHALLAXIS_CENTER_OFFSET_PX * px_adj_ratio\n",
    "    body_center_adj_y = np.sin(orientation_focus_bee) * BODY_TO_TROPHALLAXIS_CENTER_OFFSET_PX * px_adj_ratio\n",
    "\n",
    "    # add adjustment as we want to adjust to the area in front of bee 0\n",
    "    area_center_x = df_troph_video[\"x_pixels_0\"].to_numpy() + body_center_adj_x\n",
    "    area_center_y = df_troph_video[\"y_pixels_0\"].to_numpy() + body_center_adj_y\n",
    "\n",
    "    # assert center point always within frame\n",
    "    area_center_x = np.maximum(0, area_center_x)\n",
    "    area_center_x = np.minimum(video_width, area_center_x)\n",
    "    area_center_y = np.maximum(0, area_center_y)\n",
    "    area_center_y = np.minimum(video_height, area_center_y)\n",
    "\n",
    "    x_clean, y_clean = _clean_trajectory_outliers(area_center_x, area_center_y, dilation_iters=3)\n",
    "    df_troph_video[\"crop_area_center_x\"] = x_clean\n",
    "    df_troph_video[\"crop_area_center_y\"] = y_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0085585c",
   "metadata": {},
   "source": [
    "## Process the videos, cache frame and generate potential trophallaxis video clips\n",
    "\n",
    "This could still be sped up a lot by processing one original video, then caching all relevant frames for the video at once and only afterwards actually generating videos from those cached frames. I did not do this because I did not find a good way to pass a bunch of specific frame indices to ffmpeg (or a collection of ranges with one range for each clip to extract). You could simply cache all frames between first frame index to cache and last frame index to cache, which would also work and probably be faster too, but would also require more disk space because most of those frames are likely not needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vid(\n",
    "    df_troph_video: pd.DataFrame, \n",
    "    images_masked: np.ndarray, \n",
    "    video_name_short: str, \n",
    "    bee_0: int, \n",
    "    bee_1: int\n",
    ") -> str:\n",
    "    \"\"\"Generate a video clip of one potential troph. event from the images passed\n",
    "\n",
    "    Args:\n",
    "        df_troph_video (pd.DataFrame): Dataframe for the event\n",
    "        images_masked (np.ndarray): Images for the event\n",
    "        video_name_short (str): Short string from the original video (video-group + index)\n",
    "        bee_0 (int): ID of focus bee\n",
    "        bee_1 (int): ID of other bee\n",
    "\n",
    "    Returns:\n",
    "        str: Filename of the generated video clip\n",
    "    \"\"\"\n",
    "    df_troph_video_sorted = df_troph_video.sort_values(['time'])\n",
    "    sorting_indices = df_troph_video_sorted.index\n",
    "\n",
    "    start_time = df_troph_video_sorted.time.iat[0]\n",
    "    end_time = df_troph_video_sorted.time.iat[-1]\n",
    "    filename = \"_\".join((\n",
    "        video_name_short, \n",
    "        \"bees\", \n",
    "        str(bee_0), \n",
    "        \"and\", \n",
    "        str(bee_1), \n",
    "        (str(math.floor(start_time)) + \"--\" + str(math.ceil(end_time))) + \".mp4\"\n",
    "    ))\n",
    "    if GENERATE_VIDEOS:\n",
    "        images_sorted = [images_masked[i] for i in sorting_indices]\n",
    "        video_manager.write_to_video(images=images_sorted, filename=filename, frame_rate=FRAME_RATE)\n",
    "    return filename \n",
    "\n",
    "\n",
    "def process_video(\n",
    "    video_file: str, \n",
    "    df_video: pd.DataFrame, \n",
    "    generate_videos: bool\n",
    "):\n",
    "    \"\"\"Process one of the original petri dish videos\n",
    "\n",
    "    Args:\n",
    "        video_file (str): File name (incl. path) of the video\n",
    "        df_video (pd.DataFrame): Dataframe for the video\n",
    "        generate_videos (bool): Whether to generate video clips\n",
    "    \"\"\"\n",
    "\n",
    "    def process_troph_run(\n",
    "        df_bee_pair: pd.DataFrame, \n",
    "        frame_indices_run: list[int], \n",
    "        frame_id_offset: int, \n",
    "        video_width: int, \n",
    "        video_height: int, \n",
    "        px_adj_ratio_video: float, \n",
    "        video_name_short: str\n",
    "    ):\n",
    "        \"\"\"Process one of the troph. runs\n",
    "\n",
    "        Args:\n",
    "            df_bee_pair (pd.DataFrame): Dateframe for the troph. run of the bee pair\n",
    "            frame_indices_run (list[int]): Frame indices of the run (to cache)\n",
    "            frame_id_offset (int): Offset between any frame_id and frame_index in this run\n",
    "            video_width (int): Original video width in pixels\n",
    "            video_height (int): Original video height in pixels\n",
    "            px_adj_ratio_video (float): Ratio for pixel adjustment in this video compared to 1920x1080 videos\n",
    "            video_name_short (str): Short string from the original video (video-group + index)\n",
    "        \"\"\"\n",
    "\n",
    "        frame_indices_run = np.array(frame_indices_run)\n",
    "        frame_ids_run = frame_indices_run + frame_id_offset\n",
    "\n",
    "        if generate_videos:\n",
    "            video_manager.cache_frames(\n",
    "                frame_ids=frame_ids_run, \n",
    "                video_name=video_file, \n",
    "                frame_indices=frame_indices_run\n",
    "            )\n",
    "        \n",
    "        # find indices, where elements should be inserted to maintain order (possible because df_troph is sorted by frame_id)\n",
    "        start = np.searchsorted(df_bee_pair.frame_id.values, frame_ids_run[0], side=\"left\")\n",
    "        end = np.searchsorted(df_bee_pair.frame_id.values, frame_ids_run[-1], side=\"right\")\n",
    "        orientation_0_cleaned = clean_orientation_outliers(df_bee_pair.orientation_0.iloc[start:end].to_numpy())\n",
    "        df_bee_pair.iloc[start:end, df_bee_pair.columns.get_loc(\"orientation_0\")] = orientation_0_cleaned\n",
    "\n",
    "        df_troph_run = df_bee_pair.iloc[start:end].reset_index(drop=True).copy()\n",
    "        clean_trajectory_outliers(df_troph_run, video_width=video_width, video_height=video_height, px_adj_ratio=px_adj_ratio_video)\n",
    "        # smooth camera center for a more stable video\n",
    "        smooth_column_data(df_troph_run, cols_to_smooth=[\"crop_area_center_x\", \"crop_area_center_y\"], savgol_order=2)\n",
    "\n",
    "        images, metadata_value_dict = process_frames(\n",
    "            detections=df_troph_run, \n",
    "            video_manager=video_manager, \n",
    "            px_adj_ratio=px_adj_ratio_video,\n",
    "            generate_videos=generate_videos,\n",
    "            use_zoom_levels=USE_ZOOM_LEVELS,\n",
    "            use_crop_center_shift=USE_CROP_CENTER_SHIFT,\n",
    "            egocentric=USE_EGOCENTRIC_ALIGNMENT\n",
    "        )\n",
    "\n",
    "        # apply all needed masks by multiplying them to the image\n",
    "        images_masked = images if len(images) > 0 else np.array([])\n",
    "\n",
    "        filename = generate_vid(\n",
    "            df_troph_video=df_troph_run, \n",
    "            images_masked=images_masked, \n",
    "            video_name_short=video_name_short,\n",
    "            bee_0=troph_bee_0,\n",
    "            bee_1=troph_bee_1\n",
    "        )\n",
    "        \n",
    "        # overwrite json file after every video in case something goes wrong\n",
    "        with open(METADATA_OUTPUT_FILENAME, \"w\") as jsonfile:\n",
    "            metadata_dict[filename] = metadata_value_dict\n",
    "            json.dump(metadata_dict, jsonfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "    logger.info(f\"Processing video {video_file} ...\")\n",
    "    video_manager.clear_video_cache()\n",
    "    video_name_short = (str(video_file)).rsplit('/', 1)[1].split('.')[0]\n",
    "    if len(df_video) == 0:\n",
    "        logger.warning(f\"Skipped video {video_file} because dataframe was empty\")\n",
    "        return\n",
    "    \n",
    "    video_width, video_height = video_manager.get_video_dimensions(video_file)\n",
    "\n",
    "    # dataframe should already be sorted correctly but make sure\n",
    "    df_video = df_video.sort_values(['bee_id', 'frame_id'])\n",
    "    cm_per_px_video = df_video.cm_per_px.iat[0]\n",
    "    px_adj_ratio_video = DEFAULT_CM_PER_PX / cm_per_px_video\n",
    "    \n",
    "    df_video_grouped = df_video.groupby('bee_id', sort=False)\n",
    "    sizes = df_video_grouped.size()\n",
    "\n",
    "    # some video-datasets have a different number of rows for each bee, use last common frame-id\n",
    "    if sizes.nunique() != 1:\n",
    "        max_common_frame_id = df_video_grouped.frame_id.last().min()\n",
    "        df_video = df_video[df_video.frame_id <= max_common_frame_id].copy()\n",
    "\n",
    "    n_rows = len(df_video)\n",
    "    rows_per_bee = int(n_rows / 4)\n",
    "    logger.debug(f\"Total rows for video: {len(df_video)}\")\n",
    "    frame_id_offset = df_video.frame_id.iat[0] - df_video.frame_index.iat[0]\n",
    "\n",
    "    df_video = interpolate_missing_vals(df_video, cols_to_interpol=COLS_BEE_DATA)\n",
    " \n",
    "\n",
    "    troph_bee_pairs = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
    "    for pair in troph_bee_pairs:\n",
    "        logger.info(f\"Processing bee pair ({str(pair[0])}, {str(pair[1])})\")\n",
    "        # bees 0 and 1 are always the two bees considered for trophallaxis\n",
    "        troph_bee_0, troph_bee_1 = pair\n",
    "        bee_2, bee_3 = np.setdiff1d([0, 1, 2, 3], pair)\n",
    "\n",
    "        starts = np.arange(4) * rows_per_bee\n",
    "        ends = starts + rows_per_bee\n",
    "\n",
    "        cols_to_copy = COLS_BEE_DATA + ['frame_index', 'frame_id', 'video_filename', 'time']\n",
    "        # dataframe, which contains all potential trophallaxis occurances between one bee pair\n",
    "        df_bee_pair = (df_video\n",
    "                    .iloc[starts[troph_bee_0]:ends[troph_bee_0]][cols_to_copy]\n",
    "                    .rename(columns={c: f\"{c}_0\" for c in COLS_BEE_DATA})\n",
    "                    .copy())\n",
    "        for i, bee in [(1, troph_bee_1), (2, bee_2), (3, bee_3)]:\n",
    "            df_bee_pair[[f\"{c}_{i}\" for c in COLS_BEE_DATA]] = (\n",
    "                df_video\n",
    "                .iloc[starts[bee]:ends[bee]][COLS_BEE_DATA]\n",
    "                .reset_index(drop=True)\n",
    "                .to_numpy()\n",
    "            )\n",
    "\n",
    "        df_bee_pair[\"meets_troph_conds\"] = check_troph_conditions(df_bee_pair, px_adj_ratio_video)\n",
    "        df_bee_pair[\"meets_troph_conds\"] = remove_bool_islands(df_bee_pair.meets_troph_conds.to_numpy())\n",
    "        df_bee_pair[\"meets_troph_conds\"], runs_frame_indices = find_long_true_runs(df_bee_pair.meets_troph_conds.to_numpy())\n",
    "        if len(runs_frame_indices) == 0:\n",
    "            continue\n",
    "        # smooth the data for more stable videos before filtering out non-trophallaxis frames\n",
    "        cols_to_smooth = [col for i in range(4) for col in [f'x_pixels_{i}', f'y_pixels_{i}', f'orientation_{i}']]\n",
    "        smooth_column_data(df_bee_pair, cols_to_smooth=cols_to_smooth)\n",
    "        df_bee_pair = df_bee_pair.loc[df_bee_pair[\"meets_troph_conds\"] == True].copy()\n",
    "        logger.info(\"Preprocessing complete\")\n",
    "\n",
    "        for frame_indices_run in runs_frame_indices:\n",
    "            process_troph_run(\n",
    "                df_bee_pair, \n",
    "                frame_indices_run, \n",
    "                frame_id_offset, \n",
    "                video_width, \n",
    "                video_height, \n",
    "                px_adj_ratio_video, \n",
    "                video_name_short\n",
    "            )\n",
    "        \n",
    "\n",
    "metadata_dict = {}\n",
    "df_grouped = df.groupby('video_filename', sort=False)\n",
    "logger.info(f\"Videos: {df_grouped.groups.keys()}\")\n",
    "logger.info(f\"Processing {df_grouped.ngroups} videos\")\n",
    "for video_file, df_video in df_grouped:\n",
    "    process_video(video_file, df_video, GENERATE_VIDEOS)\n",
    "logger.info(\"Video generation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
